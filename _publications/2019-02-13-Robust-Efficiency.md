---
title: "Towards Efficient Methods for Training Robust Deep Neural Networks"
collection: publications
permalink: /publication/2019-02-13-Robust-Efficiency
date: 2019-02-13
venue: 'Regeneron Science Talent Search (STS) and Davidson Fellows Scholarship'
paperurl: 'https://sanjit-bhat.github.io/files/robust-efficiency19.pdf'
authors: '<strong>Sanjit&nbsp;Bhat</strong>, Dimitris&nbsp;Tsipras, Aleksander&nbsp;Mądry'
---
In recent years, it has been shown that neural networks are vulnerable to adversarial examples, i.e., specially crafted inputs that look visually similar to humans yet cause machine
learning models to make incorrect predictions. A lot of research has been focused on training
robust modelsmodels immune to adversarial examples. One such method is Adversarial Training, in which the model continuously trains on adversarially perturbed inputs. However, since
these inputs require signicant computation time to create, Adversarial Training is often much
slower than vanilla training. In this work, we explore two approaches to increase the eciency
of Adversarial Training. First, we study whether faster yet less accurate methods for generating
adversarially perturbed inputs suce to train a robust model. Second, we devise a method for
asynchronous parallel Adversarial Training and analyze a phenomenon of independent interest
that arisesstaleness. Taken together, these two techniques enable comparable robustness on
the MNIST dataset to prior art with a 26× reduction in training time from 4 hours to just 9
minutes.

[Access paper here](https://sanjit-bhat.github.io/files/robust_efficiency19.pdf)</br>
[Access presentation here](https://docs.google.com/presentation/d/1lniZ0qCE43z2bliHya_W9767t69tttDb0pauDXRJ8Rs/edit?usp=sharing)

